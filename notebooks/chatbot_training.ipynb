{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load intents data from JSON file\n",
    "with open('../data/intents.json', 'r') as file:\n",
    "    intents = json.load(file)\n",
    "\n",
    "# Extract patterns and intents labels from the data\n",
    "patterns = []\n",
    "labels = []\n",
    "\n",
    "for intent in intents['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        patterns.append(pattern)\n",
    "        labels.append(intent['tag'])\n",
    "\n",
    "# Tokenize patterns\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(patterns)\n",
    "sequences = tokenizer.texts_to_sequences(patterns)\n",
    "padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(sequences, padding='post')\n",
    "\n",
    "# Convert labels to numerical format using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Define vocabulary size and embedding dimension\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embedding_dim = 16\n",
    "\n",
    "# Build a simple neural network model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dense(len(set(encoded_labels)), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "model.fit(padded_sequences, encoded_labels, epochs=50)\n",
    "\n",
    "# Function to predict intent from user input\n",
    "def predict_intent(user_input):\n",
    "    sequence = tokenizer.texts_to_sequences([user_input])\n",
    "    padded_sequence = tf.keras.preprocessing.sequence.pad_sequences(sequence, padding='post', maxlen=padded_sequences.shape[1])\n",
    "    prediction = model.predict(padded_sequence)\n",
    "    predicted_label = np.argmax(prediction, axis=1)[0]\n",
    "    predicted_intent = label_encoder.inverse_transform([predicted_label])[0]\n",
    "    return predicted_intent\n",
    "\n",
    "# Test the chatbot with user inputs\n",
    "user_inputs = [\"hello\", \"bye\", \"thanks\", \"how are you?\"]\n",
    "\n",
    "for user_input in user_inputs:\n",
    "    intent = predict_intent(user_input)\n",
    "    print(f\"User: {user_input}\")\n",
    "    print(f\"Bot: Intent - {intent}\")\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
